{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c2410d",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸš€ Open in Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kamranjaved/Workshop-on-AI-and-Hand-on-training/blob/main/S6%3A%20CLIP%20inference%20tutorial%20/CLIP_MSCOCO_Benchmark.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab07bcc",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  CLIP Benchmark on MSCOCO 5K Test Split\n",
    "\n",
    "This notebook evaluates OpenAI's CLIP (ViT-L/14) model on the COCO Karpathy test split.\n",
    "\n",
    "**Steps:**\n",
    "1. Load the CLIP model and COCO dataset.\n",
    "2. Encode all 5,000 validation images.\n",
    "3. Perform text-to-image retrieval for 25,000 captions.\n",
    "4. Compute Recall@1, Recall@5, and Recall@10 metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6960caa",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc58593a",
   "metadata": {},
   "source": [
    "## 2. Define Paths and Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_folder = \"/home/coop2025/Documents/COOP/Sara/coco_val2014v2\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5793cd2",
   "metadata": {},
   "source": [
    "## 3. Load COCO Karpathy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"yerevann/coco-karpathy\", split=\"test\")\n",
    "print(f\"Loaded dataset with {len(dataset)} examples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953586d",
   "metadata": {},
   "source": [
    "## 4. Encode All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_features = []\n",
    "image_ids = []\n",
    "caption_to_cocoid = {}\n",
    "\n",
    "# Map each caption to its COCO image ID\n",
    "for example in dataset:\n",
    "    cocoid = example[\"cocoid\"]\n",
    "    for caption in example[\"sentences\"]:\n",
    "        caption_to_cocoid[caption] = cocoid\n",
    "\n",
    "print(\"Encoding all 5K images...\")\n",
    "for example in tqdm(dataset):\n",
    "    cocoid = example[\"cocoid\"]\n",
    "    image_path = os.path.join(image_folder, f\"COCO_val2014_000000{cocoid:06d}.jpg\")\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feat = model.encode_image(image)\n",
    "            feat /= feat.norm(dim=-1, keepdim=True)\n",
    "            image_features.append(feat.cpu())\n",
    "            image_ids.append(cocoid)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with image {cocoid}: {e}\")\n",
    "\n",
    "image_features = torch.cat(image_features)\n",
    "image_features = image_features.to(device)\n",
    "print(f\"âœ… Encoded {len(image_features)} images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b48d0",
   "metadata": {},
   "source": [
    "## 5. Perform Caption-Based Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r1 = r5 = r10 = total = 0\n",
    "\n",
    "print(\"Running caption-based image retrieval (25K queries)...\")\n",
    "for example in tqdm(dataset):\n",
    "    cocoid = example[\"cocoid\"]\n",
    "    for caption in example[\"sentences\"]:\n",
    "        prompt = f\"A photo of {caption}\"\n",
    "        text_token = clip.tokenize([prompt]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_feat = model.encode_text(text_token)\n",
    "            text_feat /= text_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        sims = (text_feat @ image_features.T).squeeze(0)\n",
    "        topk = sims.topk(10)\n",
    "        top_image_ids = [image_ids[i] for i in topk.indices.tolist()]\n",
    "\n",
    "        if cocoid == top_image_ids[0]:\n",
    "            r1 += 1\n",
    "        if cocoid in top_image_ids[:5]:\n",
    "            r5 += 1\n",
    "        if cocoid in top_image_ids[:10]:\n",
    "            r10 += 1\n",
    "        total += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be032a3",
   "metadata": {},
   "source": [
    "## 6. Report Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a67b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n===== ðŸ§¾ Final CLIP Benchmark Results on MSCOCO 5K Test Split =====\")\n",
    "print(f\"Total Queries (5 captions Ã— 5K images): {total}\")\n",
    "print(f\"Recall@1  = {r1/total:.2%}  ({r1}/{total})\")\n",
    "print(f\"Recall@5  = {r5/total:.2%}  ({r5}/{total})\")\n",
    "print(f\"Recall@10 = {r10/total:.2%}  ({r10}/{total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a6e4f",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Summary\n",
    "This notebook benchmarks the CLIP model on the MSCOCO test split.\n",
    "\n",
    "**Results Interpretation:**\n",
    "- **Recall@1**: % of times the correct image was ranked 1st for a caption.\n",
    "- **Recall@5 / @10**: % of times the correct image appeared within the top 5 or 10 results.\n",
    "\n",
    "Higher recall values indicate stronger image-text alignment.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
