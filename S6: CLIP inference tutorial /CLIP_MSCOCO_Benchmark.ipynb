{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79c2410d",
      "metadata": {
        "id": "79c2410d"
      },
      "source": [
        "\n",
        "# ðŸš€ Open in Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kamranjaved/Workshop-on-AI-and-Hand-on-training/blob/main/S6%3A%20CLIP%20inference%20tutorial%20/CLIP_MSCOCO_Benchmark.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab07bcc",
      "metadata": {
        "id": "2ab07bcc"
      },
      "source": [
        "\n",
        "# ðŸ§  CLIP Benchmark on MSCOCO 5K Test Split\n",
        "\n",
        "This notebook evaluates OpenAI's CLIP (ViT-L/14) model on the COCO Karpathy test split.\n",
        "\n",
        "**Steps:**\n",
        "1. Load the CLIP model and COCO dataset.\n",
        "2. Encode all 5,000 validation images.\n",
        "3. Perform text-to-image retrieval for 25,000 captions.\n",
        "4. Compute Recall@1, Recall@5, and Recall@10 metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6960caa",
      "metadata": {
        "id": "d6960caa"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-nU0cJU3jiIP",
        "outputId": "798a6458-5779-444d-96da-d7e05f950860"
      },
      "id": "-nU0cJU3jiIP",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-e9m9dch0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-e9m9dch0\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=896aec5bd8f11a59bb0f9561def471fc65bad209414ea5775d4f26f1e012e9f5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8dcf3q_r/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7fg_rAocke0o",
        "outputId": "620569da-c173-4063-cd95-978fc521336d"
      },
      "id": "7fg_rAocke0o",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a7a7dcf4",
      "metadata": {
        "id": "a7a7dcf4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc58593a",
      "metadata": {
        "id": "cc58593a"
      },
      "source": [
        "## 2. Define Paths and Load CLIP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9773f51a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9773f51a",
        "outputId": "d41ad885-6377-45c5-ea01-f08efc5e79a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5793cd2",
      "metadata": {
        "id": "c5793cd2"
      },
      "source": [
        "## 3. Load COCO Karpathy Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "178b5381",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "178b5381",
        "outputId": "ca205b57-d68e-4bbc-cee2-c0d383aa51bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 250 examples.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#dataset = load_dataset(\"yerevann/coco-karpathy\", split=\"test\")\n",
        "full_dataset = load_dataset(\"yerevann/coco-karpathy\", split=\"test\")\n",
        "\n",
        "# Create a new dataset containing only the first 1000 examples\n",
        "dataset = full_dataset.select(range(250))\n",
        "\n",
        "print(f\"Loaded dataset with {len(dataset)} examples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c953586d",
      "metadata": {
        "id": "c953586d"
      },
      "source": [
        "## 4. Encode All Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "466f735b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "466f735b",
        "outputId": "990e87c4-788a-4081-a1fd-3f2616884e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding all 5K images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 2946.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Encoded 2 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "image_folder = \"/content/coco_200\"\n",
        "image_features = []\n",
        "image_ids = []\n",
        "caption_to_cocoid = {}\n",
        "\n",
        "# Map each caption to its COCO image ID\n",
        "for example in dataset:\n",
        "    cocoid = example[\"cocoid\"]\n",
        "    for caption in example[\"sentences\"]:\n",
        "        caption_to_cocoid[caption] = cocoid\n",
        "\n",
        "print(\"Encoding all 5K images...\")\n",
        "for example in tqdm(dataset):\n",
        "    cocoid = example[\"cocoid\"]\n",
        "    image_path = os.path.join(image_folder, f\"COCO_val2014_000000{cocoid:06d}.jpg\")\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            feat = model.encode_image(image)\n",
        "            feat /= feat.norm(dim=-1, keepdim=True)\n",
        "            image_features.append(feat.cpu())\n",
        "            image_ids.append(cocoid)\n",
        "    except Exception as e:\n",
        "        print(f\"Error with image {cocoid}: {e}\")\n",
        "\n",
        "image_features = torch.cat(image_features)\n",
        "image_features = image_features.to(device)\n",
        "print(f\"âœ… Encoded {len(image_features)} images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05b48d0",
      "metadata": {
        "id": "e05b48d0"
      },
      "source": [
        "## 5. Perform Caption-Based Image Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sims.shape)"
      ],
      "metadata": {
        "id": "Bez2_dtMro9B",
        "outputId": "ace799ea-bb3a-45c2-faf1-efeac9ce36e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Bez2_dtMro9B",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cb74f338",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb74f338",
        "outputId": "07d51979-0590-486c-f268-110e5de529f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running caption-based image retrieval (25K queries)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:11<00:00, 22.66it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "r1 = 0\n",
        "r5 = 0\n",
        "r10 = 0\n",
        "total = 0\n",
        "\n",
        "print(\"Running caption-based image retrieval (25K queries)...\")\n",
        "for example in tqdm(dataset):\n",
        "    cocoid = example[\"cocoid\"]\n",
        "    for caption in example[\"sentences\"]:\n",
        "        prompt = f\"A photo of {caption}\"\n",
        "        text_token = clip.tokenize([prompt]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_feat = model.encode_text(text_token)\n",
        "            text_feat /= text_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        sims = (text_feat @ image_features.T).squeeze(0)\n",
        "        k = min(10, sims.size(-1))\n",
        "\n",
        "        # Now run topk with the new k\n",
        "        topk = sims.topk(k)\n",
        "        #topk = sims.topk(10)\n",
        "        top_image_ids = [image_ids[i] for i in topk.indices.tolist()]\n",
        "\n",
        "        if cocoid == top_image_ids[0]:\n",
        "            r1 += 1\n",
        "        if cocoid in top_image_ids[:5]:\n",
        "            r5 += 1\n",
        "        if cocoid in top_image_ids[:10]:\n",
        "            r10 += 1\n",
        "        total += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be032a3",
      "metadata": {
        "id": "8be032a3"
      },
      "source": [
        "## 6. Report Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "77a67b61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77a67b61",
        "outputId": "36906843-fc5f-466d-b7da-01e11816a807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ðŸ§¾ Final CLIP Benchmark Results on MSCOCO 5K Test Split =====\n",
            "Total Queries (5 captions Ã— 5K images): 1001\n",
            "Recall@1  = 0.50%  (5/1001)\n",
            "Recall@5  = 0.50%  (5/1001)\n",
            "Recall@10 = 0.50%  (5/1001)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n===== ðŸ§¾ Final CLIP Benchmark Results on MSCOCO 5K Test Split =====\")\n",
        "print(f\"Total Queries (5 captions Ã— 5K images): {total}\")\n",
        "print(f\"Recall@1  = {r1/total:.2%}  ({r1}/{total})\")\n",
        "print(f\"Recall@5  = {r5/total:.2%}  ({r5}/{total})\")\n",
        "print(f\"Recall@10 = {r10/total:.2%}  ({r10}/{total})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5a6e4f",
      "metadata": {
        "id": "7d5a6e4f"
      },
      "source": [
        "\n",
        "## âœ… Summary\n",
        "This notebook benchmarks the CLIP model on the MSCOCO test split.\n",
        "\n",
        "**Results Interpretation:**\n",
        "- **Recall@1**: % of times the correct image was ranked 1st for a caption.\n",
        "- **Recall@5 / @10**: % of times the correct image appeared within the top 5 or 10 results.\n",
        "\n",
        "Higher recall values indicate stronger image-text alignment.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}