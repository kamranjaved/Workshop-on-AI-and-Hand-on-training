{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79c2410d",
      "metadata": {
        "id": "79c2410d"
      },
      "source": [
        "\n",
        "# ðŸš€ Open in Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kamranjaved/Workshop-on-AI-and-Hand-on-training/blob/main/S6%3A%20CLIP%20inference%20tutorial%20/CLIP_MSCOCO_Benchmark.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab07bcc",
      "metadata": {
        "id": "2ab07bcc"
      },
      "source": [
        "\n",
        "# ðŸ§  CLIP Benchmark on MSCOCO 5K Test Split\n",
        "\n",
        "This notebook evaluates OpenAI's CLIP (ViT-L/14) model on the COCO Karpathy test split.\n",
        "\n",
        "**Steps:**\n",
        "1. Load the CLIP model and COCO dataset.\n",
        "2. Encode all 5,000 validation images.\n",
        "3. Perform text-to-image retrieval for 25,000 captions.\n",
        "4. Compute Recall@1, Recall@5, and Recall@10 metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6960caa",
      "metadata": {
        "id": "d6960caa"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n"
      ],
      "metadata": {
        "id": "-nU0cJU3jiIP",
        "outputId": "65e81a98-12fe-4ec7-983d-48ff75680b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-nU0cJU3jiIP",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-s4nob4wv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-s4nob4wv\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "7fg_rAocke0o",
        "outputId": "6bf52b56-304d-4641-c0e1-60854557bf74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7fg_rAocke0o",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a7a7dcf4",
      "metadata": {
        "id": "a7a7dcf4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc58593a",
      "metadata": {
        "id": "cc58593a"
      },
      "source": [
        "## 2. Define Paths and Load CLIP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9773f51a",
      "metadata": {
        "id": "9773f51a",
        "outputId": "6ed12c4e-0974-4cc9-9f4d-02905526c3a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "image_folder = \"/content/coco_val2014v2\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5793cd2",
      "metadata": {
        "id": "c5793cd2"
      },
      "source": [
        "## 3. Load COCO Karpathy Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "178b5381",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "178b5381",
        "outputId": "287ef8e2-d3a6-4fdf-fb8a-4ebcaa65cce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 200 examples.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#dataset = load_dataset(\"yerevann/coco-karpathy\", split=\"test\")\n",
        "full_dataset = load_dataset(\"yerevann/coco-karpathy\", split=\"test\")\n",
        "\n",
        "# Create a new dataset containing only the first 1000 examples\n",
        "dataset = full_dataset.select(range(200))\n",
        "\n",
        "print(f\"Loaded dataset with {len(dataset)} examples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c953586d",
      "metadata": {
        "id": "c953586d"
      },
      "source": [
        "## 4. Encode All Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "466f735b",
      "metadata": {
        "id": "466f735b",
        "outputId": "6fde19e5-ee19-4115-f7e0-b5498038c819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding all 5K images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:13<00:00, 14.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Encoded 4 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "image_features = []\n",
        "image_ids = []\n",
        "caption_to_cocoid = {}\n",
        "\n",
        "# Map each caption to its COCO image ID\n",
        "for example in dataset:\n",
        "    cocoid = example[\"cocoid\"]\n",
        "    for caption in example[\"sentences\"]:\n",
        "        caption_to_cocoid[caption] = cocoid\n",
        "\n",
        "print(\"Encoding all 5K images...\")\n",
        "for example in tqdm(dataset):\n",
        "    cocoid = example[\"cocoid\"]\n",
        "    image_path = os.path.join(image_folder, f\"COCO_val2014_000000{cocoid:06d}.jpg\")\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            feat = model.encode_image(image)\n",
        "            feat /= feat.norm(dim=-1, keepdim=True)\n",
        "            image_features.append(feat.cpu())\n",
        "            image_ids.append(cocoid)\n",
        "    except Exception as e:\n",
        "        print(f\"Error with image {cocoid}: {e}\")\n",
        "\n",
        "image_features = torch.cat(image_features)\n",
        "image_features = image_features.to(device)\n",
        "print(f\"âœ… Encoded {len(image_features)} images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05b48d0",
      "metadata": {
        "id": "e05b48d0"
      },
      "source": [
        "## 5. Perform Caption-Based Image Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cb74f338",
      "metadata": {
        "id": "cb74f338",
        "outputId": "a20d788a-6a0c-4c3d-be72-243c41e99cc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running caption-based image retrieval (25K queries)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:53<00:00,  1.47s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "r1 = r5 = r10 = total = 0\n",
        "\n",
        "print(\"Running caption-based image retrieval (25K queries)...\")\n",
        "for example in tqdm(dataset):\n",
        "    cocoid = example[\"cocoid\"]\n",
        "    for caption in example[\"sentences\"]:\n",
        "        prompt = f\"A photo of {caption}\"\n",
        "        text_token = clip.tokenize([prompt]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_feat = model.encode_text(text_token)\n",
        "            text_feat /= text_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        sims = (text_feat @ image_features.T).squeeze(0)\n",
        "        topk = sims.topk(1)\n",
        "        top_image_ids = [image_ids[i] for i in topk.indices.tolist()]\n",
        "\n",
        "        if cocoid == top_image_ids[0]:\n",
        "            r1 += 1\n",
        "        if cocoid in top_image_ids[:5]:\n",
        "            r5 += 1\n",
        "        if cocoid in top_image_ids[:10]:\n",
        "            r10 += 1\n",
        "        total += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be032a3",
      "metadata": {
        "id": "8be032a3"
      },
      "source": [
        "## 6. Report Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "77a67b61",
      "metadata": {
        "id": "77a67b61",
        "outputId": "7aa1ab83-3f97-4c4e-8da3-54a2451d4489",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ðŸ§¾ Final CLIP Benchmark Results on MSCOCO 5K Test Split =====\n",
            "Total Queries (5 captions Ã— 5K images): 1001\n",
            "Recall@1  = 1.90%  (19/1001)\n",
            "Recall@5  = 1.90%  (19/1001)\n",
            "Recall@10 = 1.90%  (19/1001)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n===== ðŸ§¾ Final CLIP Benchmark Results on MSCOCO 5K Test Split =====\")\n",
        "print(f\"Total Queries (5 captions Ã— 5K images): {total}\")\n",
        "print(f\"Recall@1  = {r1/total:.2%}  ({r1}/{total})\")\n",
        "print(f\"Recall@5  = {r5/total:.2%}  ({r5}/{total})\")\n",
        "print(f\"Recall@10 = {r10/total:.2%}  ({r10}/{total})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5a6e4f",
      "metadata": {
        "id": "7d5a6e4f"
      },
      "source": [
        "\n",
        "## âœ… Summary\n",
        "This notebook benchmarks the CLIP model on the MSCOCO test split.\n",
        "\n",
        "**Results Interpretation:**\n",
        "- **Recall@1**: % of times the correct image was ranked 1st for a caption.\n",
        "- **Recall@5 / @10**: % of times the correct image appeared within the top 5 or 10 results.\n",
        "\n",
        "Higher recall values indicate stronger image-text alignment.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}